{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1cf500",
   "metadata": {},
   "source": [
    "## Variable Acquisition\n",
    "\n",
    "### Transcriptions, PWLD (phonological distance), Word Adaptation Surprisal (orthographic distance), Average time per Response, Surprisal from Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0e9e3",
   "metadata": {},
   "source": [
    "### Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e33f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_metrics.pwld import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e79f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data\n",
    "xls = pd.ExcelFile('data/experiment_data.xlsx')\n",
    "experiment_data = {}\n",
    "# Iterate through sheet names and load into the dictionary\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    experiment_data[sheet_name] = df\n",
    "# Load experiment results\n",
    "experiment_results = pd.read_csv('data/experiment_results_raw.csv')\n",
    "experiment_results_lang = experiment_results.groupby('source_language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0892901a-1e4d-45d6-9f1b-05471ba1895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length in tokens for CS-RU: 11.3\n",
      "Average sentence length in tokens for BG-RU: 14.9\n",
      "Average sentence length in tokens for UK-RU: 14.8\n",
      "Average sentence length in tokens for BE-RU: 15.3\n",
      "Average sentence length in tokens for PL-RU: 13.6\n"
     ]
    }
   ],
   "source": [
    "# Get the average sentence length in tokens for each language\n",
    "for sheet in experiment_data:\n",
    "    print(f\"Average sentence length in tokens for {sheet}: {round(experiment_data[sheet]['sentence l2'].apply(lambda x: len(str(x).split())).mean(), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b111313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transcriptions of fixed expressions\n",
    "ru_transcription = get_phonetic_transcription(experiment_data['CS-RU']['phrase l2'].to_list(), 'RU')\n",
    "cs_transcription = get_phonetic_transcription(experiment_data['CS-RU']['phrase l2'].to_list(), 'CS')\n",
    "pl_transcription = get_phonetic_transcription(experiment_data['PL-RU']['phrase l2'].to_list(), 'PL')\n",
    "be_transcription = get_phonetic_transcription(experiment_data['BE-RU']['phrase l2'].to_list(), 'BE')\n",
    "bg_transcription = get_phonetic_transcription(experiment_data['BG-RU']['phrase l2'].to_list(), 'BG')\n",
    "uk_transcription = get_phonetic_transcription(experiment_data['UK-RU']['phrase l2'].to_list(), 'UK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transcriptions of literal options\n",
    "cs_transcription_literal = get_phonetic_transcription(experiment_data['CS-RU']['L1 literal'].to_list(), 'RU', 'cs')\n",
    "be_transcription_literal = get_phonetic_transcription(experiment_data['BE-RU']['Glosbe & Vasmer L2 (BE) - L1 (RU) '].to_list(), 'RU', 'be')\n",
    "pl_transcription_literal = get_phonetic_transcription(experiment_data['PL-RU']['Glosbe & Vasmer L2 (BE) - L1 (RU) '].to_list(), 'RU', 'pl')\n",
    "bg_transcription_literal = get_phonetic_transcription(experiment_data['BG-RU']['Glosbe & Vasmer L2 (BG) - L1 (RU) '].to_list(), 'RU', 'bg')\n",
    "uk_transcription_literal = get_phonetic_transcription(experiment_data['UK-RU']['Glosbe & Vasmer L2 (UK) - L1 (RU) '].to_list(), 'RU', 'uk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e0adc",
   "metadata": {},
   "source": [
    "### Phonologically weighted levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62674c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transcriptions of fixed expression options\n",
    "ru_transcriptions = load_transcription_dict('data/metrics/transcriptions/ru_transcriptions')\n",
    "cs_transcriptions = load_transcription_dict('data/metrics/transcriptions/cs_transcriptions')\n",
    "pl_transcriptions = load_transcription_dict('data/metrics/transcriptions/pl_transcriptions')\n",
    "uk_transcriptions = load_transcription_dict('data/metrics/transcriptions/uk_transcriptions')\n",
    "bg_transcriptions = load_transcription_dict('data/metrics/transcriptions/bg_transcriptions')\n",
    "be_transcriptions = load_transcription_dict('data/metrics/transcriptions/be_transcriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fa3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transcriptions of literal options\n",
    "cs_transcription_literal = load_transcription_dict('data/metrics/transcriptions/ru_cs_transcriptions_literal')\n",
    "be_transcription_literal = load_transcription_dict('data/metrics/transcriptions/ru_be_transcriptions_literal')\n",
    "pl_transcription_literal = load_transcription_dict('data/metrics/transcriptions/ru_pl_transcriptions_literal')\n",
    "bg_transcription_literal = load_transcription_dict('data/metrics/transcriptions/ru_bg_transcriptions_literal')\n",
    "uk_transcription_literal = load_transcription_dict('data/metrics/transcriptions/ru_uk_transcriptions_literal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe487e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics# Get transcriptions of free responses\n",
    "cs_transcription_responses = load_transcription_dict('data/metrics/transcriptions/ru_cs_transcriptions_responses')\n",
    "be_transcription_responses = load_transcription_dict('data/metrics/transcriptions/ru_be_transcriptions_responses')\n",
    "pl_transcription_responses = load_transcription_dict('data/metrics/transcriptions/ru_pl_transcriptions_responses')\n",
    "bg_transcription_responses = load_transcription_dict('data/metrics/transcriptions/ru_bg_transcriptions_responses')\n",
    "uk_transcription_responses = load_transcription_dict('data/metrics/transcriptions/ru_uk_transcriptions_responses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c312af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PWLD for fixed expression options\n",
    "for expression_transcriptions, lang in zip([cs_transcriptions, uk_transcriptions, be_transcriptions, pl_transcriptions, bg_transcriptions],\n",
    "                                           ['cs', 'uk', 'be', 'pl', 'bg']):\n",
    "    pwld_dict = get_pwld(ru_transcriptions, expression_transcriptions)\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    # Convert the dictionary to a DataFrame with separate columns for tuple elements\n",
    "    df = pd.DataFrame(pwld_dict.values(), index=pd.MultiIndex.from_tuples(pwld_dict.keys(), names=['RU', lang.upper()]), columns=['Value'])\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f'data/results/pwld/pwld_dict_fixed_ru_{lang}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0afdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PWLD for literal expression options\n",
    "for expression_transcriptions, lang in zip([(cs_transcriptions,cs_transcription_literal), (uk_transcriptions, uk_transcription_literal), (be_transcriptions, be_transcription_literal), (pl_transcriptions, pl_transcription_literal), (bg_transcriptions, bg_transcription_literal)],\n",
    "                                           ['cs', 'uk', 'be', 'pl', 'bg']):\n",
    "    pwld_dict = get_pwld( expression_transcriptions[1], expression_transcriptions[0])\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    # Convert the dictionary to a DataFrame with separate columns for tuple elements\n",
    "    df = pd.DataFrame(pwld_dict.values(), index=pd.MultiIndex.from_tuples(pwld_dict.keys(), names=['RU', lang.upper()]), columns=['Value'])\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f'data/results/pwld/pwld_dict_literal_ru_{lang}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7ca75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing invalid entries\n",
    "mappings = dict()\n",
    "for group_name, group_data in experiment_results_lang:\n",
    "    mappings[group_name] = set([(str(x).strip().lower(), str(y).strip().lower()) for x, y in zip(group_data['source_text_to_be_translated'].to_list(), group_data['user_free_translation'].to_list()) if str(y).strip().lower() not in ['nan', '']])\n",
    "\n",
    "mappings['PL'] = {(x[0] + ' rzeczy', x[1]) if x[0] == 'w gruncie' else x for x in mappings['PL']}\n",
    "mappings_part1 = dict()\n",
    "mappings_part1['BE'] = {x: be_transcriptions[x[0]] for x in mappings['BE']}\n",
    "mappings_part1['BG'] = {x: bg_transcriptions[x[0]] for x in mappings['BG']}\n",
    "mappings_part1['UK'] = {x: uk_transcriptions[x[0]] for x in mappings['UK']}\n",
    "mappings_part1['PL'] = {x: pl_transcriptions[x[0]] for x in mappings['PL']}\n",
    "mappings_part1['CS'] = {x: cs_transcriptions[x[0]] for x in mappings['CS']}\n",
    "mappings_part2 = dict()\n",
    "mappings_part2['BE'] = {x: be_transcription_responses[x[1]] for x in mappings['BE']}\n",
    "mappings_part2['BG'] = {x: bg_transcription_responses[x[1]] for x in mappings['BG']}\n",
    "mappings_part2['UK'] = {x: uk_transcription_responses[x[1]] for x in mappings['UK']}\n",
    "mappings_part2['PL'] = {x: pl_transcription_responses[x[1]] for x in mappings['PL']}\n",
    "mappings_part2['CS'] = {x: cs_transcription_responses[x[1]] for x in mappings['CS']}\n",
    "for l in mappings_part1:\n",
    "    for m1 in list(mappings_part1[l].keys()):\n",
    "        if type(mappings_part1[l][m1]) == float or type(mappings_part2[l][m1]) == float or len(mappings_part2[l][m1]) == 0 or len(mappings_part1[l][m1]) == 0 or mappings_part1[l][m1] == 'nan' or mappings_part1[l][m1] == 'nan':\n",
    "            mappings_part2[l].pop(m1)\n",
    "            mappings_part1[l].pop(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcd51c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PWLD for responses\n",
    "mappings_pwld = dict()\n",
    "for lang in mappings:\n",
    "    pwld_dict_responses = get_pwld({x: str(y) for x, y in mappings_part1[lang].items()}, {x: str(y) for x, y in mappings_part2[lang].items()})\n",
    "    pwld_dict_responses = {x[0]: y for x,y in pwld_dict_responses.items()}\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    # Convert the dictionary to a DataFrame with separate columns for tuple elements\n",
    "    df = pd.DataFrame(pwld_dict_responses.values(), index=pd.MultiIndex.from_tuples(pwld_dict_responses.keys(), names=['RU', lang.upper()]), columns=['Value'])\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f'data/results/pwld/pwld_dict_responses_ru_{lang}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827ecf4",
   "metadata": {},
   "source": [
    "### Word Adaptation Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c19870ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run the orthographic_equivalencies.py script\n",
    "%run '/get_metrics/was/Multilingual_Cloze_Tests/Distances/orthographic_equivalencies.py'\n",
    "\n",
    "# Run the utils.py script\n",
    "%run '/get_metrics/was/Multilingual_Cloze_Tests/Distances/utils.py'\n",
    "\n",
    "def get_was_ortho(dfs):\n",
    "        # Use pandas to read the Excel file and create a dictionary of DataFrames\n",
    "    dfs = pd.read_excel('data/experiment_data.xlsx', sheet_name=None)\n",
    "    \n",
    "    # 'dfs' is now a dictionary where keys are the sheet names, and values are DataFrames\n",
    "    # You can access each DataFrame by its corresponding sheet name\n",
    "    \n",
    "    # Print the sheet names\n",
    "    if 'stats' in dfs:\n",
    "        dfs.pop('stats')\n",
    "    # Access and print each DataFrame\n",
    "        dfs[sheet_name] = df[['phrase L2', 'literal translation']]\n",
    "        dfs[sheet_name].columns = sheet_name.split('-')\n",
    "        dfs[sheet_name] = dfs[sheet_name].dropna()\n",
    "    # List of language pairs\n",
    "    language_pairs = ['BG-RU', 'CS-RU', 'BE-RU', 'UK-RU', 'PL-RU']\n",
    "    results_dict = dict()\n",
    "    # Define orth_dist and other necessary functions (e.g., levenshtein_distance, character_surprisals, character_entropy, etc.)\n",
    "    \n",
    "    for lang_pair in language_pairs:\n",
    "        df = dfs[lang_pair]  # Access the DataFrame for the current language pair\n",
    "        l2 = lang_pair.split('-')[0]\n",
    "        print(l2)\n",
    "        # Compute Levenshtein distance and other metrics\n",
    "        levensthein_foreign = levenshtein_distance(df, foreign=l2, native='RU', costs=orth_dist)\n",
    "        levensthein_native = levenshtein_distance(df, foreign='RU', native=l2, costs=orth_dist)\n",
    "    \n",
    "        # Check assertions\n",
    "        assert levensthein_foreign['LD'].all() == levensthein_native['LD'].all()\n",
    "        assert levensthein_foreign['normalized LD'].all() == levensthein_native['normalized LD'].all()\n",
    "    \n",
    "        # Compute character surprisals\n",
    "        probs_foreign, surprisals_foreign = character_surprisals(levensthein_foreign, foreign=l2, native='RU')\n",
    "        probs_native, surprisals_native = character_surprisals(levensthein_native, foreign='RU', native=l2)\n",
    "    \n",
    "        # Compute character entropy\n",
    "        char_entropy_foreign = character_entropy(surprisals_foreign, probs_foreign)\n",
    "        char_entropy_native = character_entropy(surprisals_native, probs_native)\n",
    "    \n",
    "        # Compute full conditional entropy\n",
    "        H_foreign_native = full_conditional_entropy(l2, 'RU', levensthein_foreign, surprisals_native, probs_native)\n",
    "        H_native_foreign = full_conditional_entropy('RU', l2, levensthein_native, surprisals_foreign, probs_foreign)\n",
    "    \n",
    "        # Compute word adaptation surprisal\n",
    "        was_foreign = word_adaptation_surprisal(levensthein_foreign, surprisals_foreign, probs_foreign)\n",
    "        was_native = word_adaptation_surprisal(levensthein_native, surprisals_native, probs_native)\n",
    "        results_dict[lang_pair] = [was_foreign, was_native]\n",
    "    return results_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09fca3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save WAS metrics\n",
    "result_dict = get_was_ortho(experiment_data)\n",
    "for language_pair in result_dict:\n",
    "    l2 = language_pair.split('-')[0]\n",
    "    print(l2)\n",
    "    ru_l2 = result_dict[language_pair][0][[l2, 'RU', 'normalized WAS']]\n",
    "    l2_ru = result_dict[language_pair][1][[l2, 'RU', 'normalized WAS']]\n",
    "    ru_l2.to_csv(f'data/metrics/was/was_literal_ru_{l2.lower()}.csv', index=False)\n",
    "    l2_ru.to_csv(f'data/metrics/was/was_literal_{l2.lower()}_ru.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc6e21",
   "metadata": {},
   "source": [
    "### Time averaged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2c4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_times(data, time_field, averaged_time_field):\n",
    "    \"\"\"Calculate the average translation time and time per word for a given data set.\"\"\"\n",
    "    \n",
    "    averaged_times = {}\n",
    "    \n",
    "    for group_name, group_data in data:\n",
    "        relevant_data = group_data[['correct_translation', 'source_text_to_be_translated', time_field, \"source_text\"]]\n",
    "        averaged_data = relevant_data.groupby(['correct_translation','source_text_to_be_translated', \"source_text\"]).mean()\n",
    "        \n",
    "        # Calculate the average time per word\n",
    "        word_counts = [len(text.split()) for text in averaged_data.index.get_level_values('source_text')]\n",
    "        averaged_data[averaged_time_field] = averaged_data[time_field] / word_counts\n",
    "        \n",
    "        averaged_times[group_name] = averaged_data\n",
    "        \n",
    "    return averaged_times\n",
    "\n",
    "\n",
    "def save_averaged_times_to_csv(data, prefix):\n",
    "    \"\"\"Save the averaged times data to CSV files.\"\"\"\n",
    "    \n",
    "    for group_name, group_data in data.items():\n",
    "        file_name = f'data/results/time/{prefix}_{group_name.lower()}.csv'\n",
    "        group_data.to_csv(file_name, index=True)\n",
    "\n",
    "\n",
    "# Calculate average times for free translation\n",
    "average_time_taken_free = compute_average_times(experiment_results_lang, 'user_free_translation_time_taken', 'time_averaged_free')\n",
    "save_averaged_times_to_csv(average_time_taken_free, 'free')\n",
    "\n",
    "# Calculate average times for MCQ translation\n",
    "average_time_taken_mcq = compute_average_times(experiment_results_lang, 'user_mcq_translation_time_taken', 'time_averaged_free_mcq')\n",
    "save_averaged_times_to_csv(average_time_taken_mcq, 'mcq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f633f32",
   "metadata": {},
   "source": [
    "### Surprisal from LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c485d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "%run 'get_metrics/get_surprisal.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6fb00c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data/metrics/surprisal/BE_model_gpt_small_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_gpt_small_data.csv\n"
     ]
    }
   ],
   "source": [
    "#cs_df_gpt_small = get_model_df(model_gpt_small, 'CS', experiment_data, 'model_gpt_small')\n",
    "#pl_df_gpt_small = get_model_df(model_gpt_small, 'PL', experiment_data, 'model_gpt_small')\n",
    "#uk_df_gpt_small = get_model_df(model_gpt_small, 'UK', experiment_data, 'model_gpt_small')\n",
    "be_df_gpt_small = get_model_df(model_gpt_small, 'BE', experiment_data, 'model_gpt_small')\n",
    "bg_df_gpt_small = get_model_df(model_gpt_small, 'BG', experiment_data, 'model_gpt_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6562dbf8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenization mismatch for sentence 'до конца и '\n",
      "Warning: Tokenization mismatch for sentence 'povidalo se, ze predseda byl bily jako stena, opile vravoral a neco blekotal. '\n",
      "Warning: Tokenization mismatch for sentence 'presne tak, to je vyborna rada. '\n",
      "Data saved to data/metrics/surprisal/CS_model_bert_small_data.csv\n",
      "Data saved to data/metrics/surprisal/PL_model_bert_small_data.csv\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится рядом с юрием). '\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится возле с юрием). '\n",
      "Data saved to data/metrics/surprisal/UK_model_bert_small_data.csv\n",
      "Data saved to data/metrics/surprisal/BE_model_bert_small_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_bert_small_data.csv\n"
     ]
    }
   ],
   "source": [
    "cs_df_bert_small = get_model_df(model_bert_small, 'CS', experiment_data, 'model_bert_small')\n",
    "pl_df_bert_small = get_model_df(model_bert_small, 'PL', experiment_data, 'model_bert_small')\n",
    "uk_df_bert_small = get_model_df(model_bert_small, 'UK', experiment_data, 'model_bert_small')\n",
    "be_df_bert_small = get_model_df(model_bert_small, 'BE', experiment_data, 'model_bert_small')\n",
    "bg_df_bert_small = get_model_df(model_bert_small, 'BG', experiment_data, 'model_bert_small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e72f35-b687-4681-b212-48bbac3c1a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_bert_small_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_gpt_small_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_bert_large_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bg_df_bert_small = get_model_df(model_bert_small, 'BG', experiment_data, 'model_bert_small')\n",
    "bg_df_gpt_small = get_model_df(model_gpt_small, 'BG', experiment_data, 'model_gpt_small')\n",
    "bg_df_bert_large = get_model_df(model_bert_large, 'BG', experiment_data, 'model_bert_large')\n",
    "bg_df_gpt_large = get_model_df(model_gpt_large, 'BG', experiment_data, 'model_gpt_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117cc20d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenization mismatch for sentence 'до конца и '\n",
      "Warning: Tokenization mismatch for sentence 'povidalo se, ze predseda byl bily jako stena, opile vravoral a neco blekotal. '\n",
      "Warning: Tokenization mismatch for sentence 'presne tak, to je vyborna rada. '\n",
      "Data saved to data/metrics/surprisal/CS_model_bert_large_data.csv\n",
      "Data saved to data/metrics/surprisal/PL_model_bert_large_data.csv\n",
      "Warning: Tokenization mismatch for sentence 'тепер ми надаємо слово нашому головному економістові. прошу (скромно сідає поруч з юрієм). '\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится рядом с юрием). '\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится возле с юрием). '\n",
      "Warning: Tokenization mismatch for sentence 'це сталося зі мною позавчора, 3 березня, великопісної середи, у перший день після закінчення карнавалу. '\n",
      "Data saved to data/metrics/surprisal/UK_model_bert_large_data.csv\n",
      "Data saved to data/metrics/surprisal/BE_model_bert_large_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_bert_large_data.csv\n"
     ]
    }
   ],
   "source": [
    "cs_df_bert_large = get_model_df(model_bert_large, 'CS', experiment_data, 'model_bert_large')\n",
    "pl_df_bert_large = get_model_df(model_bert_large, 'PL', experiment_data, 'model_bert_large')\n",
    "uk_df_bert_large = get_model_df(model_bert_large, 'UK', experiment_data, 'model_bert_large')\n",
    "be_df_bert_large = get_model_df(model_bert_large, 'BE', experiment_data, 'model_bert_large')\n",
    "bg_df_bert_large = get_model_df(model_bert_large, 'BG', experiment_data, 'model_bert_large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ea9572e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenization mismatch for sentence 'тепер ми надаємо слово нашому головному економістові. прошу (скромно сідає поруч з юрієм). '\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится рядом с юрием). '\n",
      "Warning: Tokenization mismatch for sentence 'теперь мы предоставляем слово нашему главному экономисту. прошу (скромно садится возле с юрием). '\n",
      "Warning: Tokenization mismatch for sentence 'це сталося зі мною позавчора, 3 березня, великопісної середи, у перший день після закінчення карнавалу. '\n",
      "Data saved to data/metrics/surprisal/UK_model_gpt_large_data.csv\n",
      "Data saved to data/metrics/surprisal/BE_model_gpt_large_data.csv\n",
      "Warning: Tokenization mismatch for sentence '— не иначе как эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Warning: Tokenization mismatch for sentence '— без другого эта дрянь уронила ее где-нибудь на лужайке,  — сказал стефчов и, наклонив голову, вместе с костой принялся искать записку в траве. '\n",
      "Data saved to data/metrics/surprisal/BG_model_gpt_large_data.csv\n"
     ]
    }
   ],
   "source": [
    "cs_df_gpt_large = get_model_df(model_gpt_large, 'CS', experiment_data, 'model_gpt_large')\n",
    "pl_df_gpt_large = get_model_df(model_gpt_large, 'PL', experiment_data, 'model_gpt_large')\n",
    "uk_df_gpt_large = get_model_df(model_gpt_large, 'UK', experiment_data, 'model_gpt_large')\n",
    "be_df_gpt_large = get_model_df(model_gpt_large, 'BE', experiment_data, 'model_gpt_large')\n",
    "bg_df_gpt_large = get_model_df(model_gpt_large, 'BG', experiment_data, 'model_gpt_large')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
